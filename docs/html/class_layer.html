<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.4"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>nnlib: Layer Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">nnlib
   </div>
   <div id="projectbrief">GPU-accelerated, C/C++ neural network library.</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.4 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search",'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pub-attribs">Public Attributes</a> &#124;
<a href="class_layer-members.html">List of all members</a>  </div>
  <div class="headertitle"><div class="title">Layer Class Reference</div></div>
</div><!--header-->
<div class="contents">

<p>Represents a single layer of a neural network.  
 <a href="class_layer.html#details">More...</a></p>

<p><code>#include &lt;<a class="el" href="layer_8h_source.html">layer.h</a>&gt;</code></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-methods" name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a05e44ab517881bd3dfe9319faebb4006"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_layer.html#a05e44ab517881bd3dfe9319faebb4006">Layer</a> (size_t <a class="el" href="class_layer.html#a4ec6dd239834e32835f2cfffd0506e2d">inSize</a>, size_t <a class="el" href="class_layer.html#a0fc72a7936b05bfa0567df96b7abd08c">outSize</a>, <a class="el" href="class_activation.html">Activation</a> *<a class="el" href="class_layer.html#a5234b3bcf11a0b626b529ef23c7624db">activation</a>, <a class="el" href="vector_8h.html#a06ba0cdbafc0c9af69e3bd23b27e5ce3">DataLocation</a> <a class="el" href="class_layer.html#a248df76aa48278b0f8ca07c4d4f0a738">location</a>)</td></tr>
<tr class="memdesc:a05e44ab517881bd3dfe9319faebb4006"><td class="mdescLeft">&#160;</td><td class="mdescRight">Construct a new layer.  <a href="class_layer.html#a05e44ab517881bd3dfe9319faebb4006">More...</a><br /></td></tr>
<tr class="separator:a05e44ab517881bd3dfe9319faebb4006"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1b1ba4804451dfe6cc357194e42762ae"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_layer.html#a1b1ba4804451dfe6cc357194e42762ae">~Layer</a> ()</td></tr>
<tr class="memdesc:a1b1ba4804451dfe6cc357194e42762ae"><td class="mdescLeft">&#160;</td><td class="mdescRight">The destructor of the layer object.  <a href="class_layer.html#a1b1ba4804451dfe6cc357194e42762ae">More...</a><br /></td></tr>
<tr class="separator:a1b1ba4804451dfe6cc357194e42762ae"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:adc1b20ea056d1905a926452e7400d702"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_layer.html#adc1b20ea056d1905a926452e7400d702">forward</a> (const <a class="el" href="class_matrix.html">Matrix</a> &amp;batch)</td></tr>
<tr class="memdesc:adc1b20ea056d1905a926452e7400d702"><td class="mdescLeft">&#160;</td><td class="mdescRight">Forward one batch of data through the layer.  <a href="class_layer.html#adc1b20ea056d1905a926452e7400d702">More...</a><br /></td></tr>
<tr class="separator:adc1b20ea056d1905a926452e7400d702"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a902ce213e21837685c0d29e35417e691"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_layer.html#a902ce213e21837685c0d29e35417e691">backward</a> (const <a class="el" href="class_matrix.html">Matrix</a> &amp;delta, const <a class="el" href="class_matrix.html">Matrix</a> &amp;previousWeights, size_t batchSize=<a class="el" href="layer_8h.html#a123fa013ecd2c91686ad58c0b8d985b3">DEFAULT_BATCH_SIZE</a>, bool isLastLayer=false)</td></tr>
<tr class="memdesc:a902ce213e21837685c0d29e35417e691"><td class="mdescLeft">&#160;</td><td class="mdescRight">Backward-propagate one batch of data through the network.  <a href="class_layer.html#a902ce213e21837685c0d29e35417e691">More...</a><br /></td></tr>
<tr class="separator:a902ce213e21837685c0d29e35417e691"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a99e07431accbdbb8910a2e1cbd6852b5"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_layer.html#a99e07431accbdbb8910a2e1cbd6852b5">applyGradients</a> (size_t batchSize, <a class="el" href="allocation_8h.html#a3834ad0136a8b95a6589500e4009fe9a">DTYPE</a> learningRate=0.01)</td></tr>
<tr class="memdesc:a99e07431accbdbb8910a2e1cbd6852b5"><td class="mdescLeft">&#160;</td><td class="mdescRight">Apply the computed gradients.  <a href="class_layer.html#a99e07431accbdbb8910a2e1cbd6852b5">More...</a><br /></td></tr>
<tr class="separator:a99e07431accbdbb8910a2e1cbd6852b5"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-attribs" name="pub-attribs"></a>
Public Attributes</h2></td></tr>
<tr class="memitem:a248df76aa48278b0f8ca07c4d4f0a738"><td class="memItemLeft" align="right" valign="top"><a class="el" href="vector_8h.html#a06ba0cdbafc0c9af69e3bd23b27e5ce3">DataLocation</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_layer.html#a248df76aa48278b0f8ca07c4d4f0a738">location</a></td></tr>
<tr class="memdesc:a248df76aa48278b0f8ca07c4d4f0a738"><td class="mdescLeft">&#160;</td><td class="mdescRight">The location of the layer.  <a href="class_layer.html#a248df76aa48278b0f8ca07c4d4f0a738">More...</a><br /></td></tr>
<tr class="separator:a248df76aa48278b0f8ca07c4d4f0a738"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0fc72a7936b05bfa0567df96b7abd08c"><td class="memItemLeft" align="right" valign="top">size_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_layer.html#a0fc72a7936b05bfa0567df96b7abd08c">outSize</a></td></tr>
<tr class="memdesc:a0fc72a7936b05bfa0567df96b7abd08c"><td class="mdescLeft">&#160;</td><td class="mdescRight">The output size of the layer.  <a href="class_layer.html#a0fc72a7936b05bfa0567df96b7abd08c">More...</a><br /></td></tr>
<tr class="separator:a0fc72a7936b05bfa0567df96b7abd08c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4ec6dd239834e32835f2cfffd0506e2d"><td class="memItemLeft" align="right" valign="top">size_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_layer.html#a4ec6dd239834e32835f2cfffd0506e2d">inSize</a></td></tr>
<tr class="memdesc:a4ec6dd239834e32835f2cfffd0506e2d"><td class="mdescLeft">&#160;</td><td class="mdescRight">The input size to the layer.  <a href="class_layer.html#a4ec6dd239834e32835f2cfffd0506e2d">More...</a><br /></td></tr>
<tr class="separator:a4ec6dd239834e32835f2cfffd0506e2d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5234b3bcf11a0b626b529ef23c7624db"><td class="memItemLeft" align="right" valign="top"><a class="el" href="class_activation.html">Activation</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_layer.html#a5234b3bcf11a0b626b529ef23c7624db">activation</a></td></tr>
<tr class="memdesc:a5234b3bcf11a0b626b529ef23c7624db"><td class="mdescLeft">&#160;</td><td class="mdescRight">The activation function.  <a href="class_layer.html#a5234b3bcf11a0b626b529ef23c7624db">More...</a><br /></td></tr>
<tr class="separator:a5234b3bcf11a0b626b529ef23c7624db"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1588152248ef49963d47a8af56f89789"><td class="memItemLeft" align="right" valign="top"><a class="el" href="class_matrix.html">Matrix</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_layer.html#a1588152248ef49963d47a8af56f89789">weights</a></td></tr>
<tr class="memdesc:a1588152248ef49963d47a8af56f89789"><td class="mdescLeft">&#160;</td><td class="mdescRight">The weights of the layer.  <a href="class_layer.html#a1588152248ef49963d47a8af56f89789">More...</a><br /></td></tr>
<tr class="separator:a1588152248ef49963d47a8af56f89789"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a487f4e4d03c523b3c1b88fdb22eeaa09"><td class="memItemLeft" align="right" valign="top"><a class="el" href="class_vector.html">Vector</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_layer.html#a487f4e4d03c523b3c1b88fdb22eeaa09">biases</a></td></tr>
<tr class="memdesc:a487f4e4d03c523b3c1b88fdb22eeaa09"><td class="mdescLeft">&#160;</td><td class="mdescRight">The biases of the layer.  <a href="class_layer.html#a487f4e4d03c523b3c1b88fdb22eeaa09">More...</a><br /></td></tr>
<tr class="separator:a487f4e4d03c523b3c1b88fdb22eeaa09"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a585ee0ab655bc35af704fbf54bc5ab7b"><td class="memItemLeft" align="right" valign="top">const <a class="el" href="class_matrix.html">Matrix</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_layer.html#a585ee0ab655bc35af704fbf54bc5ab7b">data</a></td></tr>
<tr class="memdesc:a585ee0ab655bc35af704fbf54bc5ab7b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Matrix storing data passed to the layer.  <a href="class_layer.html#a585ee0ab655bc35af704fbf54bc5ab7b">More...</a><br /></td></tr>
<tr class="separator:a585ee0ab655bc35af704fbf54bc5ab7b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af0845a71fb5abcfc54a9bfc67fa771e1"><td class="memItemLeft" align="right" valign="top"><a class="el" href="class_matrix.html">Matrix</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_layer.html#af0845a71fb5abcfc54a9bfc67fa771e1">aMatrix</a></td></tr>
<tr class="memdesc:af0845a71fb5abcfc54a9bfc67fa771e1"><td class="mdescLeft">&#160;</td><td class="mdescRight">The output of the layer before applying the activation function.  <a href="class_layer.html#af0845a71fb5abcfc54a9bfc67fa771e1">More...</a><br /></td></tr>
<tr class="separator:af0845a71fb5abcfc54a9bfc67fa771e1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a241b7721e392b4a9c281e592aa76049f"><td class="memItemLeft" align="right" valign="top"><a class="el" href="class_matrix.html">Matrix</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_layer.html#a241b7721e392b4a9c281e592aa76049f">zMatrix</a></td></tr>
<tr class="memdesc:a241b7721e392b4a9c281e592aa76049f"><td class="mdescLeft">&#160;</td><td class="mdescRight">The output of the layer.  <a href="class_layer.html#a241b7721e392b4a9c281e592aa76049f">More...</a><br /></td></tr>
<tr class="separator:a241b7721e392b4a9c281e592aa76049f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4a6ffa21a68f2df1c1abdeef9e4c05ea"><td class="memItemLeft" align="right" valign="top"><a class="el" href="class_matrix.html">Matrix</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_layer.html#a4a6ffa21a68f2df1c1abdeef9e4c05ea">newDelta</a></td></tr>
<tr class="memdesc:a4a6ffa21a68f2df1c1abdeef9e4c05ea"><td class="mdescLeft">&#160;</td><td class="mdescRight">Delta that should be passed to the previous layer in the backpropagation step.  <a href="class_layer.html#a4a6ffa21a68f2df1c1abdeef9e4c05ea">More...</a><br /></td></tr>
<tr class="separator:a4a6ffa21a68f2df1c1abdeef9e4c05ea"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa1ecda24d8a36b553ac4e7261da9bdfe"><td class="memItemLeft" align="right" valign="top"><a class="el" href="class_matrix.html">Matrix</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_layer.html#aa1ecda24d8a36b553ac4e7261da9bdfe">derivatives</a></td></tr>
<tr class="memdesc:aa1ecda24d8a36b553ac4e7261da9bdfe"><td class="mdescLeft">&#160;</td><td class="mdescRight">The derivatives of the output.  <a href="class_layer.html#aa1ecda24d8a36b553ac4e7261da9bdfe">More...</a><br /></td></tr>
<tr class="separator:aa1ecda24d8a36b553ac4e7261da9bdfe"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a358267b53e907e1272570c21febc855e"><td class="memItemLeft" align="right" valign="top"><a class="el" href="class_matrix.html">Matrix</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_layer.html#a358267b53e907e1272570c21febc855e">weightsGradients</a></td></tr>
<tr class="memdesc:a358267b53e907e1272570c21febc855e"><td class="mdescLeft">&#160;</td><td class="mdescRight">The weights gradients computed by the backpropagation algorithm.  <a href="class_layer.html#a358267b53e907e1272570c21febc855e">More...</a><br /></td></tr>
<tr class="separator:a358267b53e907e1272570c21febc855e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a891c8acd17b2ca60a3a7ffffd6486946"><td class="memItemLeft" align="right" valign="top"><a class="el" href="class_vector.html">Vector</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_layer.html#a891c8acd17b2ca60a3a7ffffd6486946">biasesGradients</a></td></tr>
<tr class="memdesc:a891c8acd17b2ca60a3a7ffffd6486946"><td class="mdescLeft">&#160;</td><td class="mdescRight">The biases gradients computed by the backpropagation algorithm.  <a href="class_layer.html#a891c8acd17b2ca60a3a7ffffd6486946">More...</a><br /></td></tr>
<tr class="separator:a891c8acd17b2ca60a3a7ffffd6486946"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p >Represents a single layer of a neural network. </p>
</div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="a05e44ab517881bd3dfe9319faebb4006" name="a05e44ab517881bd3dfe9319faebb4006"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a05e44ab517881bd3dfe9319faebb4006">&#9670;&nbsp;</a></span>Layer()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">Layer::Layer </td>
          <td>(</td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>inSize</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>outSize</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="class_activation.html">Activation</a> *&#160;</td>
          <td class="paramname"><em>activation</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="vector_8h.html#a06ba0cdbafc0c9af69e3bd23b27e5ce3">DataLocation</a>&#160;</td>
          <td class="paramname"><em>location</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Construct a new layer. </p>
<p >Also allocates space that will be used during computation. This allows for in-place computation, which avoids allocating/freeing memory during training.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">inSize</td><td>The input size to the layer. </td></tr>
    <tr><td class="paramname">outSize</td><td>The output size of the layer (equal to the number of neurons). </td></tr>
    <tr><td class="paramname">activation</td><td>The activation function that should be used. </td></tr>
    <tr><td class="paramname">location</td><td>The location of the layer. See <a class="el" href="class_layer.html#a248df76aa48278b0f8ca07c4d4f0a738" title="The location of the layer.">Layer::location</a>. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a1b1ba4804451dfe6cc357194e42762ae" name="a1b1ba4804451dfe6cc357194e42762ae"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1b1ba4804451dfe6cc357194e42762ae">&#9670;&nbsp;</a></span>~Layer()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">Layer::~Layer </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">default</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>The destructor of the layer object. </p>

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="a99e07431accbdbb8910a2e1cbd6852b5" name="a99e07431accbdbb8910a2e1cbd6852b5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a99e07431accbdbb8910a2e1cbd6852b5">&#9670;&nbsp;</a></span>applyGradients()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void Layer::applyGradients </td>
          <td>(</td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>batchSize</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="allocation_8h.html#a3834ad0136a8b95a6589500e4009fe9a">DTYPE</a>&#160;</td>
          <td class="paramname"><em>learningRate</em> = <code>0.01</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Apply the computed gradients. </p>
<p >The method should be called only when all the gradients have been computed for all the layers in the network.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">batchSize</td><td>The size of the batch. </td></tr>
    <tr><td class="paramname">learningRate</td><td>The learning rate of the model. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a902ce213e21837685c0d29e35417e691" name="a902ce213e21837685c0d29e35417e691"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a902ce213e21837685c0d29e35417e691">&#9670;&nbsp;</a></span>backward()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void Layer::backward </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="class_matrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>delta</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="class_matrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>previousWeights</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>batchSize</em> = <code><a class="el" href="layer_8h.html#a123fa013ecd2c91686ad58c0b8d985b3">DEFAULT_BATCH_SIZE</a></code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>isLastLayer</em> = <code>false</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Backward-propagate one batch of data through the network. </p>
<p >Takes a boolean to specify if this layer is the output layer in the network. If it is, a slightly different algorithm must be used to compute the gradients.</p>
<p >This method only computes the gradients, it does not apply them. The gradients can only be applied once they have been calculated for all the layers. Otherwise, the passed <code>previousWeights</code> would change before the gradients have been computed. The gradients are applied in the <a class="el" href="class_layer.html#a99e07431accbdbb8910a2e1cbd6852b5" title="Apply the computed gradients.">Layer::applyGradients()</a> method.</p>
<p >Uses algorithm adapted from <a href="http://neuralnetworksanddeeplearning.com/chap2.html">http://neuralnetworksanddeeplearning.com/chap2.html</a>.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">delta</td><td><code>newDelta</code> passed from the previous layer (next in the model's architecture). </td></tr>
    <tr><td class="paramname">previousWeights</td><td>The weights of the previous layer (next in the model's architecture). </td></tr>
    <tr><td class="paramname">batchSize</td><td>The size of the batch. </td></tr>
    <tr><td class="paramname">isLastLayer</td><td>Boolean to specify if this layer is the last one (the output layer). </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="adc1b20ea056d1905a926452e7400d702" name="adc1b20ea056d1905a926452e7400d702"></a>
<h2 class="memtitle"><span class="permalink"><a href="#adc1b20ea056d1905a926452e7400d702">&#9670;&nbsp;</a></span>forward()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void Layer::forward </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="class_matrix.html">Matrix</a> &amp;&#160;</td>
          <td class="paramname"><em>batch</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Forward one batch of data through the layer. </p>
<p >This includes allocating space that could not be allocated in the constructor as it depends on the batch size. The additional data will only be allocated if batch size changes. This means, if all batches are of the same size, the data will not be allocated again. This is performed in the Layer::allocate() method.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">batch</td><td>The batch that should be propagated. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<h2 class="groupheader">Member Data Documentation</h2>
<a id="a5234b3bcf11a0b626b529ef23c7624db" name="a5234b3bcf11a0b626b529ef23c7624db"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5234b3bcf11a0b626b529ef23c7624db">&#9670;&nbsp;</a></span>activation</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="class_activation.html">Activation</a>* Layer::activation</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The activation function. </p>
<p >Pointer to the activation function object. Can be <a class="el" href="class_linear_activation.html">LinearActivation</a>, <a class="el" href="class_re_l_u_activation.html">ReLUActivation</a> or <a class="el" href="class_sigmoid_activation.html">SigmoidActivation</a>. </p>

</div>
</div>
<a id="af0845a71fb5abcfc54a9bfc67fa771e1" name="af0845a71fb5abcfc54a9bfc67fa771e1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af0845a71fb5abcfc54a9bfc67fa771e1">&#9670;&nbsp;</a></span>aMatrix</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="class_matrix.html">Matrix</a> Layer::aMatrix</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The output of the layer before applying the activation function. </p>

</div>
</div>
<a id="a487f4e4d03c523b3c1b88fdb22eeaa09" name="a487f4e4d03c523b3c1b88fdb22eeaa09"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a487f4e4d03c523b3c1b88fdb22eeaa09">&#9670;&nbsp;</a></span>biases</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="class_vector.html">Vector</a> Layer::biases</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The biases of the layer. </p>

</div>
</div>
<a id="a891c8acd17b2ca60a3a7ffffd6486946" name="a891c8acd17b2ca60a3a7ffffd6486946"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a891c8acd17b2ca60a3a7ffffd6486946">&#9670;&nbsp;</a></span>biasesGradients</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="class_vector.html">Vector</a> Layer::biasesGradients</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The biases gradients computed by the backpropagation algorithm. </p>

</div>
</div>
<a id="a585ee0ab655bc35af704fbf54bc5ab7b" name="a585ee0ab655bc35af704fbf54bc5ab7b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a585ee0ab655bc35af704fbf54bc5ab7b">&#9670;&nbsp;</a></span>data</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">const <a class="el" href="class_matrix.html">Matrix</a>* Layer::data</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Matrix storing data passed to the layer. </p>
<p >Stores a pointer reference to the batch that was most recently forward-propagated through the layer. </p>

</div>
</div>
<a id="aa1ecda24d8a36b553ac4e7261da9bdfe" name="aa1ecda24d8a36b553ac4e7261da9bdfe"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa1ecda24d8a36b553ac4e7261da9bdfe">&#9670;&nbsp;</a></span>derivatives</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="class_matrix.html">Matrix</a> Layer::derivatives</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The derivatives of the output. </p>
<p >The derivatives are computed by the activation function and stored in this variable. </p>

</div>
</div>
<a id="a4ec6dd239834e32835f2cfffd0506e2d" name="a4ec6dd239834e32835f2cfffd0506e2d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4ec6dd239834e32835f2cfffd0506e2d">&#9670;&nbsp;</a></span>inSize</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">size_t Layer::inSize</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The input size to the layer. </p>
<p >Equal to the number of neurons in the previous layer, or the size of the input in the case of the input layer. </p>

</div>
</div>
<a id="a248df76aa48278b0f8ca07c4d4f0a738" name="a248df76aa48278b0f8ca07c4d4f0a738"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a248df76aa48278b0f8ca07c4d4f0a738">&#9670;&nbsp;</a></span>location</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="vector_8h.html#a06ba0cdbafc0c9af69e3bd23b27e5ce3">DataLocation</a> Layer::location</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The location of the layer. </p>
<p >Specifies the location of all the data used by the layer. See <a class="el" href="vector_8h.html#a06ba0cdbafc0c9af69e3bd23b27e5ce3" title="Enumerate to specify where data is located.">DataLocation</a> for more info. </p>

</div>
</div>
<a id="a4a6ffa21a68f2df1c1abdeef9e4c05ea" name="a4a6ffa21a68f2df1c1abdeef9e4c05ea"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4a6ffa21a68f2df1c1abdeef9e4c05ea">&#9670;&nbsp;</a></span>newDelta</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="class_matrix.html">Matrix</a> Layer::newDelta</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Delta that should be passed to the previous layer in the backpropagation step. </p>

</div>
</div>
<a id="a0fc72a7936b05bfa0567df96b7abd08c" name="a0fc72a7936b05bfa0567df96b7abd08c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0fc72a7936b05bfa0567df96b7abd08c">&#9670;&nbsp;</a></span>outSize</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">size_t Layer::outSize</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The output size of the layer. </p>
<p >Equal to the number of neurons in the layer. </p>

</div>
</div>
<a id="a1588152248ef49963d47a8af56f89789" name="a1588152248ef49963d47a8af56f89789"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1588152248ef49963d47a8af56f89789">&#9670;&nbsp;</a></span>weights</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="class_matrix.html">Matrix</a> Layer::weights</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The weights of the layer. </p>

</div>
</div>
<a id="a358267b53e907e1272570c21febc855e" name="a358267b53e907e1272570c21febc855e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a358267b53e907e1272570c21febc855e">&#9670;&nbsp;</a></span>weightsGradients</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="class_matrix.html">Matrix</a> Layer::weightsGradients</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The weights gradients computed by the backpropagation algorithm. </p>

</div>
</div>
<a id="a241b7721e392b4a9c281e592aa76049f" name="a241b7721e392b4a9c281e592aa76049f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a241b7721e392b4a9c281e592aa76049f">&#9670;&nbsp;</a></span>zMatrix</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="class_matrix.html">Matrix</a> Layer::zMatrix</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The output of the layer. </p>

</div>
</div>
<hr/>The documentation for this class was generated from the following files:<ul>
<li>include/<a class="el" href="layer_8h_source.html">layer.h</a></li>
<li>src/ann/<a class="el" href="layer_8cpp.html">layer.cpp</a></li>
</ul>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.4
</small></address>
</body>
</html>
